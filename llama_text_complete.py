from transformers import AutoTokenizer, AutoModelForCausalLM

# Load tokenizer and model
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.2-3B-Instruct")
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-3.2-3B-Instruct")

def complete_text(prompt, max_length=256):
    """
    Generate text completion for a given prompt.
    
    Args:
        prompt (str): The input text to complete.
        max_length (int): The maximum length of the generated sequence.

    Returns:
        str: The completed text generated by the model.
    """
    # Tokenize the input prompt
    inputs = tokenizer(prompt, return_tensors="pt")
    
    # Generate text
    outputs = model.generate(
        input_ids=inputs.input_ids,
        max_length=max_length,
        num_return_sequences=1,
        temperature=0.7,
        top_k=50,
        top_p=0.95,
        do_sample=True,
    )
    
    # Decode the generated text
    completed_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return completed_text

# Example usage
if __name__ == "__main__":
    prompt = "In the near future, artificial intelligence will"
    print("Prompt:", prompt)
    print("Completion:", complete_text(prompt))
